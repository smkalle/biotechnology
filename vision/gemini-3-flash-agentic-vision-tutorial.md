# Gemini 3 Flash Agentic Vision: Complete Developer Tutorial

[![Google AI](https://img.shields.io/badge/Google-AI-4285F4?style=for-the-badge&logo=google&logoColor=white)](https://ai.google.dev/)
[![Python](https://img.shields.io/badge/Python-3.9+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg?style=for-the-badge)](https://opensource.org/licenses/Apache-2.0)

> **Transform image understanding from passive observation to active investigation with Gemini 3 Flash's revolutionary Agentic Vision capability.**

Agentic Vision is a groundbreaking capability introduced in Gemini 3 Flash (January 2026) that enables AI models to actively investigate images through code execution. Instead of processing images in a single static glance, the model can now formulate plans, zoom in, crop, annotate, and manipulate images step-by-step to ground answers in visual evidence.

---

## üìë Table of Contents

- [Overview](#overview)
- [Key Concepts](#key-concepts)
- [Prerequisites](#prerequisites)
- [Environment Setup](#environment-setup)
- [Quick Start](#quick-start)
- [Core Implementation](#core-implementation)
- [Advanced Use Cases](#advanced-use-cases)
- [API Reference](#api-reference)
- [Best Practices](#best-practices)
- [Troubleshooting](#troubleshooting)
- [Real-World Examples](#real-world-examples)
- [Performance Benchmarks](#performance-benchmarks)
- [Resources](#resources)

---

## Overview

### What is Agentic Vision?

Traditional vision models process images in a single pass. When fine details like serial numbers, distant text, or small objects are missed, the model must guess. **Agentic Vision** transforms this paradigm by treating vision as an **active investigation process**.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    AGENTIC VISION LOOP                         ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ    ‚îÇ  THINK  ‚îÇ ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ   ACT   ‚îÇ ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ OBSERVE ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ        ‚îÇ
‚îÇ         ‚ñ≤                                             ‚îÇ        ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Think: Analyze query + image, formulate multi-step plan       ‚îÇ
‚îÇ  Act: Generate & execute Python code to manipulate image       ‚îÇ
‚îÇ  Observe: Inspect transformed image, refine understanding      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Performance Impact

Enabling code execution with Gemini 3 Flash delivers:

| Benchmark Category | Quality Improvement |
|-------------------|---------------------|
| Vision Tasks | **5-10%** boost |
| Fine-grained Detection | **15-20%** accuracy increase |
| Complex Counting | **Significantly reduced** hallucinations |
| Visual Math | **Near-perfect** accuracy via code verification |

### Key Capabilities

| Capability | Description | Use Case |
|------------|-------------|----------|
| **Zoom & Inspect** | Implicitly detect small details, crop and re-examine at higher resolution | Reading serial numbers, distant gauges, fine print |
| **Image Annotation** | Draw bounding boxes, arrows, labels directly on images | Object localization, spatial reasoning |
| **Visual Math & Plotting** | Run calculations, generate Matplotlib charts from extracted data | Receipt totaling, data visualization |
| **Image Manipulation** | Crop, rotate, enhance images programmatically | Document processing, quality improvement |

---

## Key Concepts

### The Think-Act-Observe Loop

```python
"""
THINK: Model analyzes user query and initial image
       ‚Üí Formulates a multi-step investigation plan
       ‚Üí Example: "Objects are small, I need to zoom in and count"

ACT:   Model generates Python code to:
       ‚Üí Crop regions of interest
       ‚Üí Rotate/transform images  
       ‚Üí Draw annotations (bounding boxes, labels)
       ‚Üí Run calculations
       ‚Üí Generate charts/visualizations

OBSERVE: Transformed image is appended to context
         ‚Üí Model inspects new data with better context
         ‚Üí Decides if more investigation needed
         ‚Üí Generates final grounded response
"""
```

### Supported Operations

The code execution environment includes these pre-installed libraries:

```python
# Available Libraries in Code Execution Environment
AVAILABLE_LIBRARIES = [
    "altair",       # Interactive visualizations
    "chess",        # Chess board analysis
    "cv2",          # OpenCV for image processing
    "matplotlib",   # Plotting and visualization
    "mpmath",       # Arbitrary precision math
    "numpy",        # Numerical computing
    "pandas",       # Data manipulation
    "pdfminer",     # PDF text extraction
    "reportlab",    # PDF generation
    "seaborn",      # Statistical visualization
    "sklearn",      # Machine learning
    "statsmodels",  # Statistical models
    "striprtf",     # RTF processing
    "sympy",        # Symbolic mathematics
    "tabulate",     # Table formatting
]
```

> ‚ö†Ô∏è **Important**: You cannot install custom libraries. Work within the provided environment.

---

## Prerequisites

### System Requirements

- **Python**: 3.9 or higher
- **Memory**: 4GB+ RAM recommended
- **Network**: Stable internet connection for API calls

### Required Accounts & Access

1. **Google Cloud Account** with billing enabled
2. **Vertex AI API** enabled in your project
3. **API Key** from Google AI Studio OR service account credentials

### Supported Models

| Model | Agentic Vision | Context Window | Notes |
|-------|---------------|----------------|-------|
| `gemini-3-flash-preview` | ‚úÖ Full Support | 1M in / 64k out | **Recommended for Agentic Vision** |
| `gemini-3-pro-preview` | ‚úÖ Full Support | 1M in / 64k out | Higher reasoning capability |
| `gemini-2.5-flash` | ‚úÖ Supported | 1M in / 64k out | Previous generation |
| `gemini-2.5-pro` | ‚úÖ Supported | 1M in / 64k out | Previous generation |

---

## Environment Setup

### Step 1: Install the Google GenAI SDK

```bash
# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # Linux/macOS
# or
.\venv\Scripts\activate   # Windows

# Install the SDK
pip install --upgrade google-genai

# Verify installation
python -c "import google.genai; print(google.genai.__version__)"
```

### Step 2: Configure Authentication

#### Option A: Using Google AI Studio (API Key)

```bash
# Set environment variable
export GEMINI_API_KEY="your-api-key-here"
```

```python
from google import genai

# Initialize client with API key
client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))
```

#### Option B: Using Vertex AI (Service Account)

```bash
# Set environment variables for Vertex AI
export GOOGLE_CLOUD_PROJECT="your-project-id"
export GOOGLE_CLOUD_LOCATION="global"  # or specific region
export GOOGLE_GENAI_USE_VERTEXAI="True"
```

```python
from google import genai
from google.genai import types

# Initialize client for Vertex AI
client = genai.Client(
    http_options=types.HttpOptions(api_version="v1")
)
```

### Step 3: Project Structure

```
gemini-agentic-vision/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ client.py          # Client initialization
‚îÇ   ‚îú‚îÄ‚îÄ agentic_vision.py  # Core Agentic Vision functions
‚îÇ   ‚îú‚îÄ‚îÄ utils.py           # Helper utilities
‚îÇ   ‚îî‚îÄ‚îÄ examples/
‚îÇ       ‚îú‚îÄ‚îÄ zoom_inspect.py
‚îÇ       ‚îú‚îÄ‚îÄ annotation.py
‚îÇ       ‚îú‚îÄ‚îÄ visual_math.py
‚îÇ       ‚îî‚îÄ‚îÄ counting.py
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_agentic_vision.py
‚îî‚îÄ‚îÄ images/
    ‚îî‚îÄ‚îÄ sample_images/
```

### requirements.txt

```text
google-genai>=1.49.0
Pillow>=10.0.0
python-dotenv>=1.0.0
requests>=2.31.0
```

---

## Quick Start

### Minimal Working Example

```python
"""
quick_start.py - Agentic Vision in 30 lines
"""
from google import genai
from google.genai import types
import base64
import os

# Initialize client
client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

# Load an image
def load_image(path: str) -> bytes:
    with open(path, "rb") as f:
        return f.read()

# Agentic Vision analysis
def analyze_with_agentic_vision(image_bytes: bytes, prompt: str) -> dict:
    response = client.models.generate_content(
        model="gemini-3-flash-preview",
        contents=[
            types.Part.from_bytes(data=image_bytes, mime_type="image/jpeg"),
            types.Part.from_text(text=prompt)
        ],
        config=types.GenerateContentConfig(
            tools=[types.Tool(code_execution=types.ToolCodeExecution())],
            thinking_config=types.ThinkingConfig(thinking_level="HIGH"),
        ),
    )
    return response

# Example usage
if __name__ == "__main__":
    image = load_image("sample.jpg")
    result = analyze_with_agentic_vision(
        image, 
        "Count all the items in this image and label each one with a bounding box."
    )
    
    # Process response parts
    for part in result.candidates[0].content.parts:
        if part.text:
            print("Analysis:", part.text)
        if part.executable_code:
            print("Code executed:", part.executable_code.code)
        if part.code_execution_result:
            print("Output:", part.code_execution_result.output)
        if part.as_image():
            # Save annotated image
            with open("annotated_output.png", "wb") as f:
                f.write(part.as_image().image_bytes)
            print("Saved annotated image!")
```

---

## Core Implementation

### Complete Agentic Vision Module

```python
"""
agentic_vision.py - Production-ready Agentic Vision implementation
"""
from google import genai
from google.genai import types
from typing import Optional, List, Dict, Any, Union
from dataclasses import dataclass
from enum import Enum
import base64
import io
import os
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ThinkingLevel(Enum):
    """Thinking levels for Gemini 3 models"""
    MINIMAL = "minimal"  # Flash only - lowest latency
    LOW = "low"          # Quick responses
    MEDIUM = "medium"    # Flash only - balanced
    HIGH = "high"        # Maximum reasoning depth (default)


@dataclass
class AgenticVisionResult:
    """Structured result from Agentic Vision analysis"""
    status: str
    analysis_text: str
    executable_code: Optional[str]
    code_output: Optional[str]
    generated_images: List[bytes]
    thinking_content: Optional[str]
    raw_response: Any
    
    def has_images(self) -> bool:
        return len(self.generated_images) > 0
    
    def save_images(self, output_dir: str = "./output") -> List[str]:
        """Save generated images to disk"""
        os.makedirs(output_dir, exist_ok=True)
        saved_paths = []
        for i, img_bytes in enumerate(self.generated_images):
            path = os.path.join(output_dir, f"annotated_{i}.png")
            with open(path, "wb") as f:
                f.write(img_bytes)
            saved_paths.append(path)
            logger.info(f"Saved image: {path}")
        return saved_paths


class AgenticVisionClient:
    """
    High-level client for Gemini 3 Agentic Vision capabilities.
    
    Features:
    - Automatic image loading from various sources
    - Configurable thinking levels
    - Structured response parsing
    - Image output handling
    """
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        use_vertex_ai: bool = False,
        project_id: Optional[str] = None,
        location: str = "global",
    ):
        """
        Initialize the Agentic Vision client.
        
        Args:
            api_key: Google AI Studio API key (if not using Vertex AI)
            use_vertex_ai: Whether to use Vertex AI endpoint
            project_id: GCP project ID (required for Vertex AI)
            location: GCP region (default: "global")
        """
        if use_vertex_ai:
            os.environ["GOOGLE_CLOUD_PROJECT"] = project_id or os.environ.get("GOOGLE_CLOUD_PROJECT")
            os.environ["GOOGLE_CLOUD_LOCATION"] = location
            os.environ["GOOGLE_GENAI_USE_VERTEXAI"] = "True"
            self.client = genai.Client(
                http_options=types.HttpOptions(api_version="v1")
            )
        else:
            self.client = genai.Client(
                api_key=api_key or os.environ.get("GEMINI_API_KEY")
            )
        
        self.default_model = "gemini-3-flash-preview"
        logger.info(f"Initialized AgenticVisionClient (Vertex AI: {use_vertex_ai})")
    
    def load_image(
        self,
        source: Union[str, bytes],
        mime_type: str = "image/jpeg"
    ) -> types.Part:
        """
        Load image from various sources.
        
        Args:
            source: File path, URL, or raw bytes
            mime_type: Image MIME type
            
        Returns:
            types.Part ready for API call
        """
        if isinstance(source, bytes):
            image_bytes = source
        elif source.startswith(("http://", "https://")):
            import requests
            response = requests.get(source)
            response.raise_for_status()
            image_bytes = response.content
            # Auto-detect MIME type from content-type header
            content_type = response.headers.get("content-type", mime_type)
            mime_type = content_type.split(";")[0]
        else:
            with open(source, "rb") as f:
                image_bytes = f.read()
            # Auto-detect MIME type from extension
            ext = source.lower().split(".")[-1]
            mime_map = {"jpg": "image/jpeg", "jpeg": "image/jpeg", 
                       "png": "image/png", "gif": "image/gif", "webp": "image/webp"}
            mime_type = mime_map.get(ext, mime_type)
        
        return types.Part.from_bytes(data=image_bytes, mime_type=mime_type)
    
    def analyze(
        self,
        image: Union[str, bytes, types.Part],
        prompt: str,
        model: Optional[str] = None,
        thinking_level: ThinkingLevel = ThinkingLevel.HIGH,
        temperature: float = 0.5,
        max_output_tokens: int = 8192,
        include_thinking: bool = False,
    ) -> AgenticVisionResult:
        """
        Perform Agentic Vision analysis on an image.
        
        Args:
            image: Image source (path, URL, bytes, or Part)
            prompt: Analysis instructions
            model: Model ID (default: gemini-3-flash-preview)
            thinking_level: Depth of reasoning
            temperature: Response randomness (0-1)
            max_output_tokens: Maximum response length
            include_thinking: Whether to include thinking in response
            
        Returns:
            AgenticVisionResult with analysis, code, and generated images
        """
        model = model or self.default_model
        
        # Prepare image part
        if isinstance(image, types.Part):
            image_part = image
        else:
            image_part = self.load_image(image)
        
        # Build configuration
        config = types.GenerateContentConfig(
            temperature=temperature,
            max_output_tokens=max_output_tokens,
            tools=[types.Tool(code_execution=types.ToolCodeExecution())],
            thinking_config=types.ThinkingConfig(
                thinking_level=thinking_level.value,
                include_thoughts=include_thinking,
            ),
        )
        
        # Make API call
        logger.info(f"Analyzing with {model}, thinking_level={thinking_level.value}")
        response = self.client.models.generate_content(
            model=model,
            contents=[image_part, types.Part.from_text(text=prompt)],
            config=config,
        )
        
        # Parse response
        return self._parse_response(response)
    
    def analyze_multiple(
        self,
        images: List[Union[str, bytes, types.Part]],
        prompt: str,
        **kwargs
    ) -> AgenticVisionResult:
        """
        Analyze multiple images together.
        
        Args:
            images: List of image sources
            prompt: Analysis instructions
            **kwargs: Additional arguments passed to analyze()
            
        Returns:
            AgenticVisionResult
        """
        image_parts = [
            self.load_image(img) if not isinstance(img, types.Part) else img
            for img in images
        ]
        
        config = types.GenerateContentConfig(
            temperature=kwargs.get("temperature", 0.5),
            max_output_tokens=kwargs.get("max_output_tokens", 8192),
            tools=[types.Tool(code_execution=types.ToolCodeExecution())],
            thinking_config=types.ThinkingConfig(
                thinking_level=kwargs.get("thinking_level", ThinkingLevel.HIGH).value,
            ),
        )
        
        contents = image_parts + [types.Part.from_text(text=prompt)]
        
        response = self.client.models.generate_content(
            model=kwargs.get("model", self.default_model),
            contents=contents,
            config=config,
        )
        
        return self._parse_response(response)
    
    def _parse_response(self, response) -> AgenticVisionResult:
        """Parse API response into structured result"""
        analysis_parts = []
        executable_code = None
        code_output = None
        generated_images = []
        thinking_content = None
        
        for part in response.candidates[0].content.parts:
            # Skip thinking parts unless explicitly requested
            if hasattr(part, 'thought') and part.thought:
                thinking_content = part.text if part.text else None
                continue
            
            # Extract text analysis
            if part.text:
                analysis_parts.append(part.text)
            
            # Extract executable code
            if part.executable_code:
                executable_code = part.executable_code.code
                logger.debug(f"Extracted code: {executable_code[:100]}...")
            
            # Extract code execution result
            if part.code_execution_result:
                code_output = part.code_execution_result.output
            
            # Extract generated images
            img = part.as_image()
            if img is not None:
                generated_images.append(img.image_bytes)
                logger.info(f"Extracted generated image ({len(img.image_bytes)} bytes)")
        
        return AgenticVisionResult(
            status="success",
            analysis_text="\n".join(analysis_parts),
            executable_code=executable_code,
            code_output=code_output,
            generated_images=generated_images,
            thinking_content=thinking_content,
            raw_response=response,
        )


# Convenience functions for common use cases
def zoom_and_inspect(
    client: AgenticVisionClient,
    image: Union[str, bytes],
    target: str,
) -> AgenticVisionResult:
    """
    Zoom into a specific area and inspect details.
    
    Example:
        result = zoom_and_inspect(client, "control_panel.jpg", "the serial number on the gauge")
    """
    prompt = f"""
    Please carefully inspect this image and find {target}.
    If the details are too small to read clearly:
    1. Write Python code to crop that region
    2. Zoom in for better visibility
    3. Report what you find with confidence
    """
    return client.analyze(image, prompt, thinking_level=ThinkingLevel.HIGH)


def count_and_annotate(
    client: AgenticVisionClient,
    image: Union[str, bytes],
    object_type: str,
) -> AgenticVisionResult:
    """
    Count objects and annotate them with bounding boxes.
    
    Example:
        result = count_and_annotate(client, "parking_lot.jpg", "cars")
    """
    prompt = f"""
    Count all {object_type} in this image.
    To ensure accuracy:
    1. Write Python code to draw bounding boxes around each {object_type}
    2. Label each box with a number
    3. Report the total count and any observations
    
    Use matplotlib or cv2 to draw annotations on the image.
    """
    return client.analyze(image, prompt, thinking_level=ThinkingLevel.HIGH)


def extract_and_visualize(
    client: AgenticVisionClient,
    image: Union[str, bytes],
    data_type: str = "numerical data",
) -> AgenticVisionResult:
    """
    Extract data from image and create visualization.
    
    Example:
        result = extract_and_visualize(client, "receipt.jpg", "line items and prices")
    """
    prompt = f"""
    Extract {data_type} from this image.
    Then:
    1. Parse all values accurately
    2. Perform any relevant calculations
    3. Create a clear visualization (chart or annotated summary)
    4. Use Python code for all calculations to ensure accuracy
    """
    return client.analyze(image, prompt, thinking_level=ThinkingLevel.HIGH)
```

### Usage Examples

```python
"""
examples.py - Practical usage examples
"""
from agentic_vision import (
    AgenticVisionClient, 
    ThinkingLevel,
    zoom_and_inspect,
    count_and_annotate,
    extract_and_visualize
)

# Initialize client
client = AgenticVisionClient(
    api_key="your-api-key",  # or use_vertex_ai=True
)

# Example 1: Zoom and Inspect
print("=" * 50)
print("Example 1: Zoom and Inspect")
print("=" * 50)

result = zoom_and_inspect(
    client,
    "images/industrial_panel.jpg",
    "the pressure reading on the top-left gauge"
)
print(f"Analysis: {result.analysis_text}")
if result.generated_images:
    result.save_images("./output/zoom_inspect")


# Example 2: Count and Annotate
print("\n" + "=" * 50)
print("Example 2: Count and Annotate")
print("=" * 50)

result = count_and_annotate(
    client,
    "images/desk_items.jpg",
    "office supplies"
)
print(f"Analysis: {result.analysis_text}")
print(f"Code used:\n{result.executable_code}")
if result.generated_images:
    result.save_images("./output/count_annotate")


# Example 3: Extract and Visualize
print("\n" + "=" * 50)
print("Example 3: Extract Data and Create Chart")
print("=" * 50)

result = extract_and_visualize(
    client,
    "images/sales_table.png",
    "sales figures by region"
)
print(f"Analysis: {result.analysis_text}")
if result.generated_images:
    result.save_images("./output/visualization")


# Example 4: Custom Analysis with Full Control
print("\n" + "=" * 50)
print("Example 4: Custom Analysis")
print("=" * 50)

result = client.analyze(
    image="images/building_plan.pdf",
    prompt="""
    Analyze this building plan for compliance:
    1. Identify all emergency exits
    2. Measure distances between exits (if scale is provided)
    3. Highlight any areas that may not meet fire code requirements
    4. Draw annotations on the plan showing your findings
    """,
    model="gemini-3-flash-preview",
    thinking_level=ThinkingLevel.HIGH,
    temperature=0.3,
    max_output_tokens=16384,
)
print(f"Status: {result.status}")
print(f"Analysis: {result.analysis_text[:500]}...")
result.save_images("./output/building_analysis")


# Example 5: Multi-Image Comparison
print("\n" + "=" * 50)
print("Example 5: Compare Multiple Images")
print("=" * 50)

result = client.analyze_multiple(
    images=[
        "images/before.jpg",
        "images/after.jpg"
    ],
    prompt="""
    Compare these two images (before and after):
    1. Identify all differences
    2. Create a side-by-side visualization highlighting changes
    3. Quantify the changes where possible
    """,
    thinking_level=ThinkingLevel.HIGH,
)
print(f"Analysis: {result.analysis_text}")
result.save_images("./output/comparison")
```

---

## Advanced Use Cases

### 1. Building Plan Validation (Real-World Example)

PlanCheckSolver.com reported a **5% accuracy improvement** using Agentic Vision for building code compliance:

```python
"""
building_plan_validator.py - Automated building plan validation
"""

def validate_building_plan(
    client: AgenticVisionClient,
    plan_image: str,
    building_codes: str,
) -> dict:
    """
    Validate building plan against specified codes.
    
    Uses Agentic Vision to:
    1. Crop and analyze specific sections
    2. Measure distances and dimensions
    3. Verify compliance requirements
    """
    prompt = f"""
    You are a building code compliance analyst. Analyze this building plan 
    against the following requirements:
    
    {building_codes}
    
    For each requirement:
    1. Locate the relevant area in the plan
    2. If details are unclear, crop and zoom into that section
    3. Take measurements using the provided scale
    4. Annotate findings directly on the plan
    5. Provide a compliance assessment (PASS/FAIL/NEEDS REVIEW)
    
    Generate a summary image with all annotations.
    """
    
    result = client.analyze(
        image=plan_image,
        prompt=prompt,
        thinking_level=ThinkingLevel.HIGH,
        max_output_tokens=16384,
    )
    
    return {
        "status": result.status,
        "analysis": result.analysis_text,
        "annotated_plan": result.generated_images[0] if result.generated_images else None,
        "code_logic": result.executable_code,
    }

# Example usage
codes = """
1. Emergency exits must be within 75 feet of any point
2. Corridor width minimum: 44 inches
3. Exit doors must swing in direction of travel
4. Proper signage at all intersections
"""

validation = validate_building_plan(
    client,
    "office_floor_plan.pdf",
    codes
)
```

### 2. Document Data Extraction with Visualization

```python
"""
document_analyzer.py - Extract tabular data and create charts
"""

def analyze_financial_document(
    client: AgenticVisionClient,
    document: str,
) -> dict:
    """
    Extract financial data and create executive summary visualization.
    """
    prompt = """
    Analyze this financial document:
    
    1. Extract all numerical data from tables
    2. Identify key metrics (revenue, costs, margins, growth rates)
    3. Perform calculations to derive:
       - Year-over-year changes
       - Percentage breakdowns
       - Trend analysis
    4. Create a professional Matplotlib visualization showing:
       - Bar chart for absolute values
       - Line overlay for trends
       - Clear labels and legend
    5. Provide executive summary of findings
    
    Use pandas for data manipulation and matplotlib for visualization.
    All calculations must be done in code for accuracy.
    """
    
    result = client.analyze(
        image=document,
        prompt=prompt,
        thinking_level=ThinkingLevel.HIGH,
    )
    
    return {
        "summary": result.analysis_text,
        "charts": result.generated_images,
        "calculation_code": result.executable_code,
        "raw_output": result.code_output,
    }
```

### 3. Interactive Object Detection and Labeling

```python
"""
object_labeler.py - Detect and label objects with confidence scores
"""

def detect_and_label_objects(
    client: AgenticVisionClient,
    image: str,
    object_categories: List[str],
    min_confidence: float = 0.7,
) -> AgenticVisionResult:
    """
    Detect objects, draw bounding boxes, and label with confidence.
    """
    categories_str = ", ".join(object_categories)
    
    prompt = f"""
    Detect all objects in this image belonging to these categories: {categories_str}
    
    For each detected object:
    1. Draw a bounding box around it
    2. Label with: category name and confidence level
    3. Use color coding: Green (>90%), Yellow (70-90%), Red (<70%)
    4. Create a detection summary table
    
    Code requirements:
    - Use cv2 or matplotlib for drawing
    - Output the annotated image
    - Print a summary: category, count, average confidence
    
    Only report objects with confidence >= {min_confidence}
    """
    
    return client.analyze(
        image=image,
        prompt=prompt,
        thinking_level=ThinkingLevel.HIGH,
    )
```

### 4. Multi-Turn Conversational Analysis

```python
"""
chat_vision.py - Multi-turn image analysis conversation
"""

class AgenticVisionChat:
    """
    Maintain conversation context for iterative image analysis.
    """
    
    def __init__(self, client: AgenticVisionClient):
        self.client = client
        self.history = []
        self.current_image = None
    
    def set_image(self, image: Union[str, bytes]):
        """Set the image for analysis"""
        self.current_image = self.client.load_image(image)
        self.history = []
    
    def ask(self, question: str) -> AgenticVisionResult:
        """
        Ask a question about the current image, maintaining context.
        """
        if not self.current_image:
            raise ValueError("No image set. Call set_image() first.")
        
        # Build conversation history
        contents = [self.current_image]
        for turn in self.history:
            contents.append(types.Part.from_text(text=turn["user"]))
            contents.append(types.Part.from_text(text=turn["assistant"]))
        contents.append(types.Part.from_text(text=question))
        
        # Make request
        config = types.GenerateContentConfig(
            tools=[types.Tool(code_execution=types.ToolCodeExecution())],
            thinking_config=types.ThinkingConfig(thinking_level="HIGH"),
        )
        
        response = self.client.client.models.generate_content(
            model=self.client.default_model,
            contents=contents,
            config=config,
        )
        
        result = self.client._parse_response(response)
        
        # Update history
        self.history.append({
            "user": question,
            "assistant": result.analysis_text,
        })
        
        return result

# Usage
chat = AgenticVisionChat(client)
chat.set_image("complex_diagram.png")

# First question
result1 = chat.ask("What is this diagram showing?")
print(result1.analysis_text)

# Follow-up with context
result2 = chat.ask("Can you zoom into the bottom-right section and explain it in detail?")
print(result2.analysis_text)
result2.save_images("./output")

# Another follow-up
result3 = chat.ask("Now create a simplified version of just that section")
print(result3.analysis_text)
```

---

## API Reference

### Configuration Options

#### GenerateContentConfig

```python
config = types.GenerateContentConfig(
    # Core settings
    temperature=0.5,                    # 0-1, controls randomness
    max_output_tokens=8192,             # Maximum response length
    top_p=0.95,                         # Nucleus sampling threshold
    top_k=40,                           # Top-k sampling
    
    # Agentic Vision - REQUIRED
    tools=[
        types.Tool(code_execution=types.ToolCodeExecution())
    ],
    
    # Thinking configuration
    thinking_config=types.ThinkingConfig(
        thinking_level="HIGH",          # minimal, low, medium, high
        # OR legacy:
        # thinking_budget=2048,         # Token budget for thinking
        include_thoughts=False,         # Include thinking in response
    ),
    
    # Safety settings (optional)
    safety_settings=[
        types.SafetySetting(
            category="HARM_CATEGORY_DANGEROUS_CONTENT",
            threshold="BLOCK_ONLY_HIGH"
        )
    ],
)
```

#### Thinking Levels

| Level | Description | Latency | Use Case |
|-------|-------------|---------|----------|
| `minimal` | Almost no thinking (Flash only) | Lowest | Simple queries, high throughput |
| `low` | Minimal reasoning | Low | Simple tasks, chat |
| `medium` | Balanced (Flash only) | Medium | Most tasks |
| `high` | Maximum reasoning (default) | High | Complex analysis, Agentic Vision |

#### Media Resolution

```python
# Control token usage for images
config = types.GenerateContentConfig(
    # ... other settings ...
)

# Per-image resolution control
image_part = types.Part.from_bytes(
    data=image_bytes,
    mime_type="image/jpeg",
)
# Set resolution at part level (v1alpha API)
# media_resolution={"level": "media_resolution_high"}

# Resolution options:
# - media_resolution_low: 280 tokens
# - media_resolution_medium: 560 tokens  
# - media_resolution_high: 1120 tokens
# - media_resolution_ultra_high: Maximum quality
```

### Response Structure

```python
# Response object structure
response.candidates[0].content.parts = [
    # Text part
    Part(
        text="Analysis results...",
        thought=False,  # True if this is thinking content
    ),
    
    # Executable code part
    Part(
        executable_code=ExecutableCode(
            language="PYTHON",
            code="import matplotlib...",
        )
    ),
    
    # Code execution result part
    Part(
        code_execution_result=CodeExecutionResult(
            outcome="OUTCOME_OK",  # or OUTCOME_ERROR
            output="Printed output...",
        )
    ),
    
    # Generated image part
    Part(
        inline_data=Blob(
            mime_type="image/png",
            data=b"...",  # Raw bytes
        )
    ),
]

# Helper method for images
for part in response.candidates[0].content.parts:
    img = part.as_image()  # Returns Image object or None
    if img:
        img.image_bytes    # Raw bytes
        img.save("out.png")  # Save to file
```

---

## Best Practices

### 1. Prompt Engineering for Agentic Vision

```python
# ‚ùå Vague prompt - model may not use code execution
prompt = "What's in this image?"

# ‚úÖ Specific prompt that triggers agentic behavior
prompt = """
Analyze this image and:
1. Identify all objects that appear to be electronics
2. For any items with text/labels that are hard to read, 
   crop and zoom in to read them accurately
3. Draw bounding boxes around each identified item
4. Create a summary table with: Item, Brand (if visible), Condition
5. Use Python code for all image manipulation to ensure accuracy
"""
```

### 2. Temperature and Thinking Settings

```python
# For analytical tasks (counting, measurements)
analytical_config = types.GenerateContentConfig(
    temperature=0.2,  # Lower = more deterministic
    thinking_config=types.ThinkingConfig(thinking_level="HIGH"),
    tools=[types.Tool(code_execution=types.ToolCodeExecution())],
)

# For creative tasks (generating visualizations)
creative_config = types.GenerateContentConfig(
    temperature=0.7,  # Higher = more creative
    thinking_config=types.ThinkingConfig(thinking_level="MEDIUM"),
    tools=[types.Tool(code_execution=types.ToolCodeExecution())],
)
```

### 3. Error Handling

```python
from google.api_core import exceptions

def safe_analyze(client, image, prompt):
    """Robust analysis with error handling"""
    try:
        result = client.analyze(image, prompt)
        
        # Check for code execution errors
        if result.code_output and "Error" in result.code_output:
            logger.warning(f"Code execution warning: {result.code_output}")
        
        return result
        
    except exceptions.ResourceExhausted:
        logger.error("Rate limit exceeded. Implement backoff.")
        raise
        
    except exceptions.InvalidArgument as e:
        logger.error(f"Invalid request: {e}")
        raise
        
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        raise
```

### 4. Token Management

```python
def estimate_tokens(image_bytes: bytes, text: str) -> int:
    """
    Rough token estimation for planning.
    
    - Image: ~258 tokens base + varies by resolution
    - Text: ~4 chars per token
    """
    image_tokens = 258  # Base for standard resolution
    text_tokens = len(text) // 4
    return image_tokens + text_tokens

# Check before making expensive calls
estimated = estimate_tokens(my_image, my_prompt)
if estimated > 100000:
    logger.warning(f"Large request: ~{estimated} tokens")
```

### 5. Batch Processing

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def batch_analyze(
    client: AgenticVisionClient,
    images: List[str],
    prompt: str,
    max_concurrent: int = 5
) -> List[AgenticVisionResult]:
    """
    Process multiple images with controlled concurrency.
    """
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def analyze_one(image):
        async with semaphore:
            # Run in thread pool since SDK is synchronous
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as pool:
                return await loop.run_in_executor(
                    pool,
                    lambda: client.analyze(image, prompt)
                )
    
    tasks = [analyze_one(img) for img in images]
    return await asyncio.gather(*tasks)

# Usage
results = asyncio.run(batch_analyze(client, image_list, "Analyze this image"))
```

---

## Troubleshooting

### Common Errors and Solutions

#### 1. `from_image_bytes` Does Not Exist

```python
# ‚ùå Incorrect (method doesn't exist)
types.Part.from_image_bytes(data=image_data, mime_type="image/png")

# ‚úÖ Correct method
types.Part.from_bytes(data=image_data, mime_type="image/png")
```

#### 2. `ThinkingLevel` Enum Not Found

```python
# ‚ùå May not exist in older SDK versions
types.ThinkingLevel.MEDIUM

# ‚úÖ Use string value directly
types.ThinkingConfig(thinking_level="MEDIUM")

# ‚úÖ Or use integer budget (legacy)
types.ThinkingConfig(thinkingBudget=2048)
```

#### 3. Truncated Responses

**Problem**: Analysis cut off mid-sentence.

**Cause**: Thinking consumes `max_output_tokens` budget.

```python
# ‚ùå Thinking uses most tokens
config = types.GenerateContentConfig(
    max_output_tokens=2048,  # Too low!
)

# ‚úÖ Increase tokens or disable thinking for simple tasks
config = types.GenerateContentConfig(
    max_output_tokens=8192,  # Increased
    thinking_config=types.ThinkingConfig(
        thinkingBudget=0  # Disable thinking if not needed
    ),
)
```

#### 4. Code Execution Timeout

**Problem**: Complex code times out (30 second limit).

**Solution**: Simplify requested operations:

```python
# ‚ùå Too complex for single execution
prompt = "Process all 1000 images in this folder..."

# ‚úÖ Break into smaller tasks
prompt = "Analyze this single image and create one visualization..."
```

#### 5. No Images Generated

**Problem**: `result.generated_images` is empty.

**Causes**:
- Prompt didn't explicitly request image output
- Code execution failed silently

```python
# ‚úÖ Explicitly request image output
prompt = """
...
Make sure to:
1. Save the annotated image using plt.savefig() or cv2.imwrite()
2. Display the image so it appears in the output
"""
```

#### 6. Rate Limiting

```python
import time
from tenacity import retry, wait_exponential, stop_after_attempt

@retry(
    wait=wait_exponential(multiplier=1, min=4, max=60),
    stop=stop_after_attempt(5)
)
def analyze_with_retry(client, image, prompt):
    return client.analyze(image, prompt)
```

### Debug Mode

```python
def debug_response(result: AgenticVisionResult):
    """Print detailed response information for debugging"""
    print("=" * 60)
    print("DEBUG: AgenticVisionResult")
    print("=" * 60)
    print(f"Status: {result.status}")
    print(f"Analysis length: {len(result.analysis_text)} chars")
    print(f"Has executable code: {result.executable_code is not None}")
    print(f"Has code output: {result.code_output is not None}")
    print(f"Number of images: {len(result.generated_images)}")
    
    if result.executable_code:
        print("\n--- Executable Code ---")
        print(result.executable_code[:500] + "..." if len(result.executable_code) > 500 else result.executable_code)
    
    if result.code_output:
        print("\n--- Code Output ---")
        print(result.code_output[:500] + "..." if len(result.code_output) > 500 else result.code_output)
    
    print("=" * 60)
```

---

## Real-World Examples

### Example 1: Receipt Processing

```python
"""
Process a receipt image: extract items, calculate total, verify
"""
result = client.analyze(
    image="receipt.jpg",
    prompt="""
    Process this receipt:
    1. Extract all line items with prices
    2. Calculate subtotal, tax, and total
    3. Verify the printed total matches your calculation
    4. Create a summary visualization showing the breakdown
    5. Flag any discrepancies
    
    Use pandas for data organization and matplotlib for visualization.
    """
)

print(result.analysis_text)
if result.generated_images:
    result.save_images("./receipts/processed")
```

### Example 2: Inventory Counting

```python
"""
Count items on a warehouse shelf
"""
result = count_and_annotate(
    client,
    "warehouse_shelf.jpg",
    "boxes"
)

# Output includes:
# - Exact count with confidence
# - Annotated image with numbered boxes
# - Any counting methodology notes
```

### Example 3: Technical Diagram Analysis

```python
"""
Analyze a circuit diagram
"""
result = client.analyze(
    image="circuit.png",
    prompt="""
    Analyze this circuit diagram:
    1. Identify all components (resistors, capacitors, etc.)
    2. Trace the signal flow
    3. Annotate each component with its value if visible
    4. Create a component list/BOM
    5. Highlight any potential issues or improvements
    
    Draw annotations directly on the circuit diagram.
    """
)
```

---

## Performance Benchmarks

### Latency Comparison

| Task | Without Agentic Vision | With Agentic Vision | Notes |
|------|----------------------|---------------------|-------|
| Simple description | 1-2s | 3-5s | Code execution adds overhead |
| Object counting | 1-2s | 5-8s | Much more accurate |
| Data extraction | 2-3s | 8-15s | Verified calculations |
| Complex annotation | 2-3s | 10-20s | Returns annotated image |

### Accuracy Improvements

| Task | Standard Vision | Agentic Vision | Improvement |
|------|----------------|----------------|-------------|
| Fine text reading | 75% | 95% | +20% |
| Counting objects | 80% | 98% | +18% |
| Numerical calculations | 60% | 99% | +39% |
| Spatial reasoning | 70% | 90% | +20% |

---

## Resources

### Official Documentation

- [Introducing Agentic Vision in Gemini 3 Flash](https://blog.google/innovation-and-ai/technology/developers-tools/agentic-vision-gemini-3-flash/) - Google Blog
- [Gemini 3 Developer Guide](https://ai.google.dev/gemini-api/docs/gemini-3) - API Documentation
- [Code Execution Documentation](https://ai.google.dev/gemini-api/docs/code-execution) - Detailed API Reference
- [Vertex AI Code Execution](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/code-execution) - Cloud Documentation

### Interactive Tools

- [Google AI Studio Demo](https://aistudio.google.com/apps/bundled/gemini_visual_thinking) - Try Agentic Vision
- [AI Studio Playground](https://aistudio.google.com/prompts/new_chat?model=gemini-3-flash-preview) - Experiment with Code Execution

### SDK Resources

- [google-genai Python SDK](https://github.com/googleapis/python-genai) - GitHub Repository
- [SDK Reference Documentation](https://googleapis.github.io/python-genai/) - API Reference

### Community Examples

- [Gemini Cookbook](https://github.com/google-gemini/cookbook) - Official Examples
- [Vertex AI Samples](https://github.com/GoogleCloudPlatform/generative-ai) - Cloud Samples

---

## Contributing

Contributions are welcome! Please feel free to submit issues and pull requests.

### Development Setup

```bash
git clone https://github.com/your-repo/gemini-agentic-vision-tutorial.git
cd gemini-agentic-vision-tutorial
pip install -e ".[dev]"
pytest tests/
```

---

## License

This tutorial is licensed under the Apache 2.0 License. See [LICENSE](LICENSE) for details.

---

## Changelog

### v1.0.0 (January 2026)
- Initial release
- Core Agentic Vision implementation
- Examples for zoom, annotate, visualize use cases
- Comprehensive API reference

---

<p align="center">
  <b>Built with ‚ù§Ô∏è for the AI developer community</b>
  <br>
  <a href="https://ai.google.dev">Google AI</a> ‚Ä¢
  <a href="https://cloud.google.com/vertex-ai">Vertex AI</a> ‚Ä¢
  <a href="https://aistudio.google.com">AI Studio</a>
</p>
